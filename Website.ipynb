{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Segmentation\n",
    "# Part 1: Image Classification\n",
    "\n",
    "## Spec parameters\n",
    "Using a simple two layer convolution neural network andthe hyperparameters suggested from the spec, I was able to get the test accuracy increased around 90%.\n",
    "\n",
    "### Structure of the CNN:\n",
    "![](part1Net1.png)\n",
    "### Training and testing Accuracy:\n",
    "![](accuracy_naive.jpg)\n",
    "\n",
    "## Improved Model:\n",
    "Using a deeper and wider CNN with batch normalization, the test accuracy can be further improved to 92.5%:\n",
    "### Structure of the CNN:\n",
    "![](part1Net2.png)\n",
    "### Training and Testing Loss/Accuracy:\n",
    "\n",
    "Loss | Accuracy\n",
    ":-: | :-: \n",
    "![](loss.jpg) | ![](accuracy.jpg)\n",
    "\n",
    "## Accuracy of Different Catagory\n",
    "Accuracy of T-shirt top : 90%  \n",
    "Accuracy of Trouser : 97%  \n",
    "Accuracy of Pullover : 92%  \n",
    "Accuracy of Dress : 94%  \n",
    "Accuracy of  Coat : 81%  \n",
    "Accuracy of Sandal : 98%  \n",
    "Accuracy of Shirt : 72%  \n",
    "Accuracy of Sneaker : 95%  \n",
    "Accuracy of   Bag : 98%  \n",
    "Accuracy of Ankle boot : 97%  \n",
    "\n",
    "It performs the worst on shirt. I think it is because all the pictures are only 28 by 28 pixels, making the buttons and other features of the shirt is very hard to distinguish with other clothes, each as coat and T-shirt, which also has low accuracy.\n",
    "\n",
    "\n",
    "## Example of Correct and Incorrect Predictions\n",
    "<table width=\"100%\">\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Class</td>\n",
    "      <td width=\"40%\" style=\"text-align:center\">Correct Predictions</td>\n",
    "      <td width=\"40%\" style=\"text-align:center\">Incorrect Predictions</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">T-shirt top</td>\n",
    "      <td width=\"40%\"><img src=\"T-shirt top_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"T-shirt top_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Trouser</td>\n",
    "      <td width=\"40%\"><img src=\"Trouser_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Trouser_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Pullover</td>\n",
    "      <td width=\"40%\"><img src=\"Pullover_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Pullover_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Dress</td>\n",
    "      <td width=\"40%\"><img src=\"Dress_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Dress_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Coat</td>\n",
    "      <td width=\"40%\"><img src=\"Coat_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Coat_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Sandal</td>\n",
    "      <td width=\"40%\"><img src=\"Sandal_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Sandal_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Shirt</td>\n",
    "      <td width=\"40%\"><img src=\"Shirt_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Shirt_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Sneaker</td>\n",
    "      <td width=\"40%\"><img src=\"Sneaker_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Sneaker_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Bag</td>\n",
    "      <td width=\"40%\"><img src=\"Bag_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Bag_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "      <td width=\"20%\" style=\"text-align:center\">Ankle boot</td>\n",
    "      <td width=\"40%\"><img src=\"Ankle boot_correct.jpg\" width=\"300px\" />\n",
    "      <td width=\"40%\"><img src=\"Ankle boot_incorrect.jpg\" width=\"300px\" /></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Filter visualization\n",
    "The below image is the visualized channels of the first layer of the CNN:\n",
    "![](conv1.jpg)\n",
    "\n",
    "As we have learned different convolution kernels, the trained kernels seem to extract blurry edges, contours, and boundaries from the original graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Semantic Segmentation\n",
    "## CNN Design\n",
    "For hyperparameters, I use CrossEntropy loss functions and AdamOptimizer with learning rate of 0.003 and weight decay of 0.0001 for all models.\n",
    "### Simple Approch\n",
    "By trying with different parameters and different layers of Conv2d, I use 5 Conv2d with increasing channels(64-128-256-512-1024) and padding of 1 with kernel size of 3.  \n",
    "To restore the channels and the resolution, the final layer is a conv2dTranspose layer with 5 output channels with kernel and stride of size 8.\n",
    "\n",
    "#### Training and Validation Loss\n",
    "![](double_net_64Loss.jpg)\n",
    "\n",
    "#### Results\n",
    "\n",
    "**CLASS** | **AP**\n",
    ":-- | ---\n",
    "others | 0.6828809643454818  \n",
    "facade | 0.7903953947401656  \n",
    "pillar | 0.23578283817459583  \n",
    "window | 0.854123292949606  \n",
    "balcony | 0.5521705116866344  \n",
    "**Average Test AP** | 0.6230706003792967  \n",
    "\n",
    "#### Detailed Structure\n",
    "![](simpleNet.png)\n",
    "\n",
    "### Best Approch\n",
    "Inspired by the [U-Net](https://arxiv.org/abs/1505.04597), I use a very similar but shallower CNN model to get the best result. Instead of only 5 convolution layers, there are total of 16 convolution layers with batch normalization to get the best result. The channel starts at 64 and maximize at 256. Also, for the purpose of symmetry, their are 3 conv2dTranspose layers with kernel and stride of 2 to upscale the resolution back to 256x256 instead of using only 1 layer.\n",
    "\n",
    "#### Training and Validation Loss\n",
    "![](complex_net_64Loss.jpg)\n",
    "\n",
    "#### Results\n",
    "\n",
    "**CLASS** | **AP**\n",
    ":-- | ---\n",
    "others | 0.7603433060379275\n",
    "facade | 0.8369236856459152\n",
    "pillar | 0.30451121379695517\n",
    "window | 0.8817632233245439\n",
    "balcony | 0.7031278175049809\n",
    "**Average Test AP** | 0.6973338492620645\n",
    "\n",
    "#### Detailed Structure\n",
    "![](complexNet.png)\n",
    "\n",
    "### Examples\n",
    "Below are some example result of the network from the test set and one of my images:\n",
    "\n",
    "**Original** | **Result**\n",
    ":-- | ---\n",
    "![](x100.png) | ![](y100.png)\n",
    "![](x22.png) | ![](y22.png)\n",
    "![](x2.png) | ![](y2.png)\n",
    "![](custom.jpg) | ![](customSeg.png)\n",
    "\n",
    "The first two example are very good. However, the last from the test set and my own image perform not so well because they have a more complex light effect and very different color channels due to the light effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
